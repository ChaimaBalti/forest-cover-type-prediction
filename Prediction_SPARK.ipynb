{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Forest Cover Type Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Introduction**\n",
    "\n",
    "This notebook documents a project focused on predicting forest cover types using the [Kaggle dataset: Forest Cover Type Prediction Dataset](https://www.kaggle.com/competitions/forest-cover-type-prediction/data)\n",
    ". The project is part of the **Big Data** module of ENIT's 3rd year MIndS and is undertaken by **Group 4**: Chaima Balti, Roukaya Lakhzouri, and Salsabil Rouahi. We are working under the supervision of our professor, **Moez Ben Haj Hmida**.\n",
    "\n",
    "The primary goal of this project is to explore and apply various machine learning techniques to accurately classify forest cover types based on specific features related to soil, climate, and topography. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Libraries** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, corr\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Start a Spark Session\n",
    "spark = SparkSession.builder.appName(\"LR_with_PySpark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = \"train.csv\"  # Replace with the actual path to your dataset\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Logistic regression (multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter correlations with the target\n",
    "target_column = \"Cover_Type\"  # Replace with your actual target column\n",
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test Accuracy: 66.08%\n",
      "Best Hyperparameters:\n",
      " - Regularization Parameter (regParam): 0.01\n",
      " - ElasticNet Parameter (elasticNetParam): 0.5\n",
      " - Max Iterations (maxIter): 300\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Assuming df is your DataFrame with feature columns and 'Cover_Type' as the target\n",
    "\n",
    "# Step 1: Combine feature columns into a single vector\n",
    "feature_columns = [col for col in df.columns if col != \"Cover_Type\"]\n",
    "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df = vector_assembler.transform(df)\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 3: Initialize the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Cover_Type\", family=\"multinomial\")\n",
    "\n",
    "# Step 4: Build the parameter grid\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(lr.regParam, [0.01])  # Regularization parameter\n",
    "              .addGrid(lr.elasticNetParam, [0.5])  # ElasticNet mixing parameter\n",
    "              .addGrid(lr.maxIter, [300])  # Maximum number of iterations\n",
    "              .build())\n",
    "\n",
    "# Step 5: Define the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Step 6: Set up CrossValidator\n",
    "cross_validator = CrossValidator(estimator=lr,\n",
    "                                  estimatorParamMaps=param_grid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=5)  # 5-fold cross-validation\n",
    "\n",
    "# Step 7: Fit the model with CrossValidator\n",
    "cv_model = cross_validator.fit(train_df)\n",
    "\n",
    "# Step 8: Get the best model\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Step 9: Evaluate the best model on the test set\n",
    "predictions = best_model.transform(test_df)\n",
    "test_accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Best Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 10: Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\" - Regularization Parameter (regParam): {best_model._java_obj.getRegParam()}\")\n",
    "print(f\" - ElasticNet Parameter (elasticNetParam): {best_model._java_obj.getElasticNetParam()}\")\n",
    "print(f\" - Max Iterations (maxIter): {best_model._java_obj.getMaxIter()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle '_thread.RLock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(best_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpark_model.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\site-packages\\joblib\\numpy_pickle.py:553\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 553\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    555\u001b[0m     NumpyPickler(filename, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mstart_framing()\n\u001b[1;32m--> 487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(obj)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(STOP)\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mend_framing()\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj)\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo to six elements\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_reduce(obj\u001b[38;5;241m=\u001b[39mobj, \u001b[38;5;241m*\u001b[39mrv)\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state_setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         save(state)\n\u001b[0;32m    718\u001b[0m         write(BUILD)\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    720\u001b[0m         \u001b[38;5;66;03m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;66;03m# to update obj's with its previous state.\u001b[39;00m\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;66;03m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[0;32m    723\u001b[0m         \u001b[38;5;66;03m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj)\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[38;5;28mself\u001b[39m, obj)  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(MARK \u001b[38;5;241m+\u001b[39m DICT)\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize(obj)\n\u001b[1;32m--> 972\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_setitems(obj\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tmp:\n\u001b[0;32m    997\u001b[0m         save(k)\n\u001b[1;32m--> 998\u001b[0m         save(v)\n\u001b[0;32m    999\u001b[0m     write(SETITEMS)\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m n:\n",
      "    \u001b[1;31m[... skipping similar frames: NumpyPickler.save at line 355 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo to six elements\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_reduce(obj\u001b[38;5;241m=\u001b[39mobj, \u001b[38;5;241m*\u001b[39mrv)\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state_setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         save(state)\n\u001b[0;32m    718\u001b[0m         write(BUILD)\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    720\u001b[0m         \u001b[38;5;66;03m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;66;03m# to update obj's with its previous state.\u001b[39;00m\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;66;03m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[0;32m    723\u001b[0m         \u001b[38;5;66;03m# (obj, state) onto the stack.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: NumpyPickler.save at line 355 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[38;5;28mself\u001b[39m, obj)  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(MARK \u001b[38;5;241m+\u001b[39m DICT)\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize(obj)\n\u001b[1;32m--> 972\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_setitems(obj\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tmp:\n\u001b[0;32m    997\u001b[0m         save(k)\n\u001b[1;32m--> 998\u001b[0m         save(v)\n\u001b[0;32m    999\u001b[0m     write(SETITEMS)\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m n:\n",
      "    \u001b[1;31m[... skipping similar frames: NumpyPickler.save at line 355 (5 times), _Pickler._batch_setitems at line 998 (2 times), _Pickler.save at line 603 (2 times), _Pickler.save at line 560 (2 times), _Pickler.save_dict at line 972 (2 times), _Pickler.save_reduce at line 717 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo to six elements\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_reduce(obj\u001b[38;5;241m=\u001b[39mobj, \u001b[38;5;241m*\u001b[39mrv)\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state_setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         save(state)\n\u001b[0;32m    718\u001b[0m         write(BUILD)\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    720\u001b[0m         \u001b[38;5;66;03m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;66;03m# to update obj's with its previous state.\u001b[39;00m\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;66;03m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[0;32m    723\u001b[0m         \u001b[38;5;66;03m# (obj, state) onto the stack.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: NumpyPickler.save at line 355 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[38;5;28mself\u001b[39m, obj)  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(MARK \u001b[38;5;241m+\u001b[39m DICT)\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize(obj)\n\u001b[1;32m--> 972\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_setitems(obj\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tmp:\n\u001b[0;32m    997\u001b[0m         save(k)\n\u001b[1;32m--> 998\u001b[0m         save(v)\n\u001b[0;32m    999\u001b[0m     write(SETITEMS)\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m n:\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj)\n",
      "File \u001b[1;32mc:\\Users\\balti_j80n85d\\anaconda3\\envs\\CudafriendlyENV\\Lib\\pickle.py:578\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    576\u001b[0m reduce \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce_ex__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 578\u001b[0m     rv \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    580\u001b[0m     reduce \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle '_thread.RLock' object"
     ]
    }
   ],
   "source": [
    "joblib.dump(best_model, 'Spark_model.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CudafriendlyENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
